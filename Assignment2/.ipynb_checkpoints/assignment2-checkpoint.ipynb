{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training files and test files, initialize spark and some parameters\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('Cloud Computing Assignment2') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "train_datafile = 'hdfs://soit-hdp-pro-1.ucc.usyd.edu.au/share/MNIST/Train-28x28.csv'\n",
    "train_labelfile= 'hdfs://soit-hdp-pro-1.ucc.usyd.edu.au/share/MNIST/Train-label.csv'\n",
    "test_datafile = 'hdfs://soit-hdp-pro-1.ucc.usyd.edu.au/share/MNIST/Test-28x28.csv'\n",
    "test_labelfile= 'hdfs://soit-hdp-pro-1.ucc.usyd.edu.au/share/MNIST/Test-label.csv'\n",
    "num_train_samples = 60000\n",
    "num_test_samples = 10000\n",
    "\n",
    "train_data = spark.read.csv(train_datafile,header=False,inferSchema=\"true\")\n",
    "train_label = spark.read.csv(train_labelfile,header=False,inferSchema=\"true\")\n",
    "test_data = spark.read.csv(test_datafile,header=False,inferSchema=\"true\")\n",
    "test_label = spark.read.csv(test_labelfile,header=False,inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data files into vectors and the label files into nparray\n",
    "assembler1 = VectorAssembler(inputCols=train_data.columns,outputCol=\"train_features\")\n",
    "train_vectors = assembler1.transform(train_data).select(\"train_features\")\n",
    "\n",
    "assembler2 = VectorAssembler(inputCols=train_label.columns,outputCol=\"train_labels\")\n",
    "train_label_vectors = assembler2.transform(train_label).select(\"train_labels\")\n",
    "train_label_array = np.array(train_label_vectors.collect())\n",
    "train_label_array=train_label_array.reshape(num_train_samples)\n",
    "\n",
    "assembler3 = VectorAssembler(inputCols=test_data.columns,outputCol=\"test_features\")\n",
    "test_vectors = assembler3.transform(test_data).select(\"test_features\")\n",
    "\n",
    "assembler4 = VectorAssembler(inputCols=test_label.columns,outputCol=\"test_labels\")\n",
    "test_label_vectors = assembler4.transform(test_label).select(\"test_labels\")\n",
    "test_label_array = np.array(test_label_vectors.collect())\n",
    "test_label_array=test_label_array.reshape(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k = 84, inputCol=\"train_features\", outputCol=\"train_pca\")\n",
    "trainmodel = pca.fit(train_vectors)\n",
    "train_pca_result = trainmodel.transform(train_vectors).select('train_pca')\n",
    "\n",
    "pca2 = PCA(k = 84, inputCol=\"test_features\", outputCol=\"test_pca\")\n",
    "testmodel = pca2.fit(test_vectors)\n",
    "test_pca_result = testmodel.transform(test_vectors).select('test_pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca_array = np.array(train_pca_result.collect())\n",
    "train_pca_array=train_pca_array.reshape(num_train_samples,84)\n",
    "\n",
    "test_pca_array = np.array(test_pca_result.collect())\n",
    "test_pca_array=test_pca_array.reshape(num_test_samples,84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=49, algorithm='auto').fit(train_pca_array)\n",
    "distances, indices = nbrs.kneighbors(test_pca_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7., 0., 1., 0., 0., 1., 0., 2., 5., 7.])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = []\n",
    "for i in indices:\n",
    "    result_labels = {}\n",
    "    for j in i:\n",
    "        if train_label_array[j] in result_labels.keys():\n",
    "            result_labels[train_label_array[j]] += 1\n",
    "        else:\n",
    "            result_labels[train_label_array[j]] = 1\n",
    "    test_result.append(max(result_labels, key=result_labels.get))\n",
    "test_result = np.array(test_result)\n",
    "test_result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2356"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count = 0\n",
    "for i in np.arange(num_test_samples):\n",
    "    if test_result[i]==test_label_array[i]:\n",
    "        correct_count += 1\n",
    "acc = correct_count/10000\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
